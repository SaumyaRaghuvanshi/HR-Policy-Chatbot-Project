# -*- coding: utf-8 -*-
"""hr_policy_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BFm8HnQDVLYPv5uLLerTB8ppDcy3vYHZ
"""

# Commented out IPython magic to ensure Python compatibility.
"""
HR Policy Assistant Chatbot ‚Äî Streamlit App
-------------------------------------------------
Features
- PDF uploader + default sample loader (optional)
- PDF text extraction, cleaning, and chunking
- FAISS vector store with Sentence-Transformers embeddings
- Retrieval-Augmented Generation (RAG) with Groq LLMs
- Sources cited inline (chunk page numbers + snippets)
- Auto visualizations: LLM can emit a viz_spec JSON block that we parse to render Plotly charts/tables
- Employee feedback capture (Yes/No), saved to CSV and used to lightly boost future retrievals
- Clean, modular, beginner-friendly code structure

Usage
- Set environment variable GROQ_API_KEY before running
- Run:  streamlit run hr_policy_assistant_streamlit_app.py

Dependencies (install once)
  pip install streamlit pypdf sentence-transformers faiss-cpu plotly pandas groq tiktoken

Note: If faiss-cpu install fails on some platforms, try: pip install faiss-cpu==1.7.4
"""
# %pip install streamlit pypdf sentence-transformers faiss-cpu plotly pandas groq tiktoken

import os
import io
import json
import time
import uuid
import base64
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple, Optional


import streamlit as st
import pandas as pd
import plotly.express as px


from sentence_transformers import SentenceTransformer
import numpy as np


try:
  import faiss # type: ignore
except Exception as e:
  faiss = None


from pypdf import PdfReader
from groq import Groq

# ------------------------------
# Configuration
# ------------------------------
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2" # fast + small + free
DEFAULT_TOP_K = 5
CHUNK_SIZE = 900 # characters
CHUNK_OVERLAP = 200 # characters
INDEX_DIR = "./indexes"
FEEDBACK_LOG = "feedback_log.csv"
ANSWER_LOG = "answer_log.csv"

def ensure_dirs():
  os.makedirs(INDEX_DIR, exist_ok=True)

def hash_bytes(data: bytes) -> str:
  return hashlib.sha256(data).hexdigest()

def pdf_to_text(pdf_bytes: bytes) -> Tuple[str, List[int]]:
  """Extract raw text from PDF bytes. Returns (text, page_starts) where page_starts[i] is char index of page i start."""
  reader = PdfReader(io.BytesIO(pdf_bytes))
  texts = []
  page_starts = []
  cursor = 0
  for page in reader.pages:
    page_starts.append(cursor)
    t = page.extract_text() or ""
    # Normalize whitespace
    t = "\n".join(line.strip() for line in t.splitlines())
    texts.append(t)
    cursor += len(t) + 1
  full = "\n".join(texts)
  return full, page_starts

def find_page_for_offset(page_starts: List[int], offset: int) -> int:
  # binary search-ish
  lo, hi = 0, len(page_starts) - 1
  while lo <= hi:
    mid = (lo + hi) // 2
    if page_starts[mid] <= offset:
      lo = mid + 1
    else:
      hi = mid - 1
  return max(0, hi)

def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[Tuple[str, int]]:
  """Return list of (chunk, start_char_index)."""
  chunks = []
  i = 0
  n = len(text)
  while i < n:
    j = min(n, i + chunk_size)
    chunks.append((text[i:j], i))
    i = j - overlap
    if i <= 0:
      i = j
    return chunks

@dataclass
class ChunkMeta:
  id: str
  doc_id: str
  start: int
  end: int
  page: int
  preview: str

class SimpleVectorIndex:
  """A light FAISS wrapper storing vectors + metadata in local folder."""

  def __init__(self, index_path: str, dim: int):
    self.index_path = index_path
    self.dim = dim
    self.meta_path = index_path + ".meta.json"
    self.docmap_path = index_path + ".docs.json"
    self.id_map: List[str] = [] # chunk id order aligns with faiss ids
    self.meta: Dict[str, ChunkMeta] = {}
    self.docmap: Dict[str, Dict[str, Any]] = {}
    self.faiss_index = None


  def _create_faiss(self):
    if faiss is None:
      raise RuntimeError("FAISS is not installed. Please install faiss-cpu.")
    self.faiss_index = faiss.IndexFlatIP(self.dim)


  def save(self):
    if self.faiss_index is None:
      return
    faiss.write_index(self.faiss_index, self.index_path)
    with open(self.meta_path, "w", encoding="utf-8") as f:
      json.dump({k: asdict(v) for k, v in self.meta.items()}, f, ensure_ascii=False, indent=2)
    with open(self.docmap_path, "w", encoding="utf-8") as f:
      json.dump(self.docmap, f, ensure_ascii=False, indent=2)
    with open(self.index_path + ".ids", "w", encoding="utf-8") as f:
      json.dump(self.id_map, f)


  def load(self):
    if not (os.path.exists(self.index_path) and os.path.exists(self.meta_path) and os.path.exists(self.index_path + ".ids")):
      return False
    if faiss is None:
      raise RuntimeError("FAISS is not installed. Please install faiss-cpu.")
    self.faiss_index = faiss.read_index(self.index_path)
    with open(self.meta_path, "r", encoding="utf-8") as f:
      meta_dict = json.load(f)
      self.meta = {k: ChunkMeta(**v) for k, v in meta_dict.items()}
    with open(self.docmap_path, "r", encoding="utf-8") as f:
      self.docmap = json.load(f)
    with open(self.index_path + ".ids", "r", encoding="utf-8") as f:
      self.id_map = json.load(f)
    self.dim = self.faiss_index.d
    return True


  def add(self, vectors: np.ndarray, metas: List[ChunkMeta]):
    if self.faiss_index is None:
      self._create_faiss()
    # Normalize for cosine similarity as inner product
    norms = np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10
    vectors = vectors / norms
    self.faiss_index.add(vectors.astype("float32"))
    for m in metas:
      self.id_map.append(m.id)
      self.meta[m.id] = m

  def search(self, query_vec: np.ndarray, top_k: int = DEFAULT_TOP_K) -> List[Tuple[ChunkMeta, float]]:
    if self.faiss_index is None:
      return []
    q = query_vec / (np.linalg.norm(query_vec) + 1e-10)
    D, I = self.faiss_index.search(q.astype("float32").reshape(1, -1), top_k)
    results = []
    for score, idx in zip(D[0], I[0]):
      if idx == -1:
        continue
      cid = self.id_map[idx]
      results.append((self.meta[cid], float(score)))
    return results

# ------------------------------
# Embeddings
# ------------------------------
@st.cache_resource(show_spinner=False)
def get_embedder():
  return SentenceTransformer(EMBED_MODEL_NAME)


def embed_texts(model: SentenceTransformer, texts: List[str]) -> np.ndarray:
  embs = model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=False)
  return embs

# ------------------------------
# LLM (Groq)
# ------------------------------
@st.cache_resource(show_spinner=False)
def get_groq_client():
  api_key = os.getenv("GROQ_API_KEY", "")
  if not api_key:
    st.warning("GROQ_API_KEY not set. Set it to enable answers.")
  return Groq(api_key=api_key) if api_key else None




def call_llm_groq(client: Groq, system_prompt: str, user_prompt: str, model: str = "llama-3.1-8b-instant") -> str:
  """Call Groq Chat Completions API and return text."""
  if client is None:
    return "[LLM disabled: please set GROQ_API_KEY]"
  completion = client.chat.completions.create(
    messages=[
      {"role": "system", "content": system_prompt},
      {"role": "user", "content": user_prompt},
    ],
    model=model,
    temperature=0.2,
    max_tokens=1200,
)
  return completion.choices[0].message.content or ""

# ------------------------------
# Feedback store
# ------------------------------


def append_csv(path: str, row: Dict[str, Any]):
  df = pd.DataFrame([row])
  header = not os.path.exists(path)
  df.to_csv(path, mode="a", header=header, index=False, encoding="utf-8")


def load_feedback_weights() -> Dict[str, float]:
  """Compute simple per-doc weighting from feedback to bias retrieval.
  We increase weight for doc_id with positive feedback and decrease for negative.
  """
  if not os.path.exists(FEEDBACK_LOG):
    return {}
  df = pd.read_csv(FEEDBACK_LOG)
  weights: Dict[str, float] = {}
  for _, r in df.iterrows():
    doc_id = str(r.get("doc_id", ""))
    helpful = str(r.get("helpful", "")).lower() == "yes"
    if not doc_id:
      continue
      weights.setdefault(doc_id, 0.0)
      weights[doc_id] += 0.5 if helpful else -0.25
      # Convert to multiplicative weights in [0.5, 2.0]
  for k, v in list(weights.items()):
    weights[k] = float(np.clip(1.0 + v, 0.5, 2.0))
  return weights

# ------------------------------
# RAG core
# ------------------------------
SYSTEM_PROMPT = (
"You are an HR Policy Assistant for employees. Answer clearly, cite policy sources with section/page, "
"and if the user asks for structured insights, include a JSON code block named viz_spec with keys: "
"type(one of 'bar','line','pie','table'), title, data(list of objects), x, y, group(optional), table_columns(optional). "
"If creating charts, keep categories concise. Never fabricate policy details; use only supplied CONTEXT."
)


def build_rag_prompt(question: str, hits: List[Tuple[ChunkMeta, float]], full_text: str) -> Tuple[str, List[Dict[str, Any]]]:
  # Prepare a compact context with citations
  context_parts = []
  srcs = []
  for meta, score in hits:
    snippet = full_text[meta.start:meta.end]
    snippet = snippet.replace("\n", " ")
    context_parts.append(f"[Source id={meta.id} page={meta.page+1} score={score:.3f}] {snippet}")
    srcs.append({"id": meta.id, "page": meta.page + 1, "score": score, "preview": meta.preview})
  context = "\n\n".join(context_parts)
  user_prompt = f"CONTEXT:\n{context}\n\nUSER QUESTION: {question}\n\nGuidelines: If policy isn't explicit in the context, say so and suggest next steps. After the answer, list the sources used as [id @ page]. If quantitative categories are present (grades, leave days, limits), also output a viz_spec JSON code block."
  return user_prompt, srcs

# ------------------------------
# Streamlit UI helpers
# ------------------------------


def render_sources(srcs: List[Dict[str, Any]]):
  if not srcs:
    return
  with st.expander("Sources (from uploaded PDFs)"):
    for s in srcs:
      st.markdown(f"‚Ä¢ **{s['id']}** ‚Äî page {s['page']} ‚Äî _{s['preview']}_ (score {s['score']:.2f})")


def try_render_viz(answer_text: str):
  """Parse a fenced code block ```json viz_spec ...``` or ```viz_spec ...```; render Plotly/table if found."""
  import re
  pattern = re.compile(r"```(?:json)?\s*viz_spec\s*(\{[\s\S]*?\})\s*```", re.IGNORECASE)
  m = pattern.search(answer_text or "")
  if not m:
    return False
  try:
    spec = json.loads(m.group(1))
  except Exception:
    st.info("Found a viz_spec block but couldn't parse it.")
  return False


  vtype = spec.get("type", "table").lower()
  title = spec.get("title", "")
  data = spec.get("data", [])
  x = spec.get("x")
  y = spec.get("y")
  group = spec.get("group")
  table_cols = spec.get("table_columns")


  df = pd.DataFrame(data)
  if not df.empty:
    if vtype == "bar":
      fig = px.bar(df, x=x, y=y, color=group, title=title)
      st.plotly_chart(fig, use_container_width=True)
    elif vtype == "line":
      fig = px.line(df, x=x, y=y, color=group, markers=True, title=title)
      st.plotly_chart(fig, use_container_width=True)
    elif vtype == "pie":
      fig = px.pie(df, names=x, values=y, title=title)
      st.plotly_chart(fig, use_container_width=True)
    elif vtype == "table":
      if table_cols:
        df = df[table_cols]
      st.dataframe(df, use_container_width=True)
    else:
      st.dataframe(df, use_container_width=True)
    return True
  return False

# ------------------------------
# Main App
# ------------------------------


def main():
  st.set_page_config(page_title="HR Policy Assistant", page_icon="üß≠", layout="wide")
  st.title("üß≠ HR Policy Assistant Chatbot")
  st.caption("Ask interactive questions about HR policies. Upload your HR PDF or use the default. Answers cite sources and can produce charts/tables.")

  ensure_dirs()

  # Sidebar: Build/Load Index
  with st.sidebar:
    st.header("üìÑ Documents & Index")
    uploaded_files = st.file_uploader("Upload HR policy PDFs", type=["pdf"], accept_multiple_files=True)
    use_sample = st.checkbox("Load sample HR_Policy.pdf if found in working directory", value=False)
    index_name = st.text_input("Index name", value="hr_policy_index")
    build_btn = st.button("(Re)Build Index", type="primary")
    st.divider()
    st.header("‚öôÔ∏è Settings")
    top_k = st.slider("Top-K retrieval", 3, 10, DEFAULT_TOP_K)
    model_choice = st.selectbox("Groq model", ["llama-3.1-8b-instant", "mixtral-8x7b-32768"], index=0)

  index_path = os.path.join(INDEX_DIR, f"{index_name}.faiss")

  if build_btn:
    with st.spinner("Building index..."):
      embedder = get_embedder()
      vec_dim = embedder.get_sentence_embedding_dimension()
      index = SimpleVectorIndex(index_path, vec_dim)
      if os.path.exists(index_path):
        # Fresh rebuild
        for p in [index_path, index_path + ".meta.json", index_path + ".docs.json", index_path + ".ids"]:
          try:
            os.remove(p)
          except FileNotFoundError:
            pass

      # Collect PDFs
      datas: List[Tuple[str, bytes]] = []
      if uploaded_files:
        for f in uploaded_files :
          datas.append((f.name, f.read()))
      if use_sample and not uploaded_files:
          # Attempt to load a local sample
          sample_path = "HR_Policy.pdf"
          if os.path.exists(sample_path):
            with open(sample_path, "rb") as f:
              datas.append((os.path.basename(sample_path), f.read()))
          else:
            st.warning("Sample HR_Policy.pdf not found in current directory.")
      if not datas:
        st.error("Please upload at least one PDF or enable the sample.")
        st.stop()

     # Init index
      vec_dim = embedder.get_sentence_embedding_dimension()
      index = SimpleVectorIndex(index_path, vec_dim)
      index._create_faiss()

      for fname, fbytes in datas:
        doc_id = hash_bytes(fbytes)[:12]
        raw_text, page_starts = pdf_to_text(fbytes)
        chunks = chunk_text(raw_text, CHUNK_SIZE, CHUNK_OVERLAP)
        texts = [c[0] for c in chunks]
        starts = [c[1] for c in chunks]
        ends = [s + len(t) for s, t in zip(starts, texts)]
        metas: List[ChunkMeta] = []
        for t, s, e in zip(texts, starts, ends):
          cid = str(uuid.uuid4())[:8]
          page = find_page_for_offset(page_starts, s)
          preview = (t.strip().split("\n")[0] or t)[:120]
          metas.append(ChunkMeta(id=cid, doc_id=doc_id, start=s, end=e, page=page, preview=preview))
          vectors = embed_texts(embedder, texts)
          index.add(vectors, metas)
          # Save docmap entry
          index.docmap[doc_id] = {"filename": fname, "pages": len(page_starts)}


      index.save()
      st.success("Index built and saved.")

  # Chat area
  st.subheader("üí¨ Ask about HR policies")
  question = st.text_input("Your question", placeholder="e.g., What is the earned leave policy and how much can I encash?")
  ask_btn = st.button("Ask")

  if ask_btn and question.strip():
    # Load index + metadata
    embedder = get_embedder()
    index = SimpleVectorIndex(index_path, embedder.get_sentence_embedding_dimension())
    if not index.load():
      st.error("No index found. Please (re)build the index from the sidebar.")
      st.stop()

    # Search
    qvec = embed_texts(embedder, [question])[0]
    hits = index.search(qvec, top_k=top_k)

    # Apply simple feedback weights per doc
    weights = load_feedback_weights()
    if weights:
      for i, (m, s) in enumerate(hits):
        w = weights.get(m.doc_id, 1.0)
        hits[i] = (m, s * w)
      # re-sort
      hits.sort(key=lambda x: x[1], reverse=True)

    # Re-compose full text for snippets
    # We need the original PDF text to slice previews; the simplest: store raw text per doc in docmap? To keep app light,
    # reconstruct by asking user to include sample doc; we'll read first doc bytes from current working dir if present.
    # For accurate citations, we keep only snippet preview that was saved during indexing and avoid re-slicing.
    # To allow the LLM to see full context, we include whole chunk texts again by reconstructing from previews is insufficient.
    # Pragmatic approach: store chunk spans already in meta and pull from a cache in session when we built. If not present, show previews only.

    # Lightweight cache: when building, we didn't persist raw_text due to size. We'll ask the user to rebuild index in same session for perfect snippets.
    # As a fallback, we concatenate previews as context (still useful for LLM). In a production app, persist raw_text per doc.
    full_text_map: Dict[str, str] = st.session_state.get("_doc_text_map", {})
    if not full_text_map:
      st.info("Tip: Build the index in this session to enable longer exact snippets. Using short previews for now.")

    # Assemble context lines
    context_lines = []
    srcs = []
    for meta, score in hits:
      if full_text_map and meta.doc_id in full_text_map:
        full_text = full_text_map[meta.doc_id]
        snippet = full_text[meta.start:meta.end]
      else:
        snippet = meta.preview
        context_lines.append(f"[Source id={meta.id} page={meta.page+1} score={score:.3f}] {snippet}")
        srcs.append({"id": meta.id, "page": meta.page + 1, "score": score, "preview": meta.preview, "doc_id": meta.doc_id})

    context = "\n\n".join(context_lines)

    user_prompt = (
        f"CONTEXT (policy excerpts):\n{context}\n\n"
        f"USER QUESTION: {question}\n\n"
        "Instructions: Answer using only the CONTEXT. Cite sources as [id @ page]. "
        "If counts or categories are mentioned (leave types, limits, grades, reimbursements), also produce a fenced JSON code block labelled viz_spec."
    )

    client = get_groq_client()
    answer = call_llm_groq(client, SYSTEM_PROMPT, user_prompt, model=model_choice)

    st.markdown("### Answer")
    st.write(answer)
    rendered = try_render_viz(answer)
    render_sources(srcs)

    # Feedback UI
    st.markdown("#### Was this answer helpful?")
    col1, col2 = st.columns(2)
    with col1:
      if st.button("üëç Yes"):
        append_csv(FEEDBACK_LOG, {
         "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
         "question": question,
         "helpful": "Yes",
         "doc_id": srcs[0]["doc_id"] if srcs else "",
         "sources": json.dumps(srcs, ensure_ascii=False),
        })
        st.success("Thanks for your feedback!")
    with col2:
      if st.button("üëé No"):
        append_csv(FEEDBACK_LOG, {
         "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
         "question": question,
         "helpful": "No",
         "doc_id": srcs[0]["doc_id"] if srcs else "",
         "sources": json.dumps(srcs, ensure_ascii=False),
         })
        st.info("Feedback noted ‚Äî we'll use it to improve retrieval.")

    # Log the full Q/A for auditing
    append_csv(ANSWER_LOG, {
        "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        "question": question,
        "answer": answer,
        "rendered_viz": rendered,
        "sources": json.dumps(srcs, ensure_ascii=False),
    })


  # Helpful tips
  with st.expander("üìò Tips & Examples"):
    st.markdown(
        "- *Policy lookup:* `What are office hours and the public holiday rules?`\n"
        "- *Leave:* `Explain earned leave vs. half pay leave; can I carry forward or encash?`\n"
        "- *Benefits:* `What is the domestic travel DA policy for Level 6?`\n"
        "- *Visualization:* `Show earned leave entitlement by staff grade as a bar chart.`\n"
        "- *Code of conduct:* `What are the do's and don'ts under conduct rules?`\n"
        )


  st.caption("Built for MBA project demos ‚Ä¢ Local FAISS + Groq LLM ‚Ä¢ Data stays on your machine (except LLM calls)")


if __name__ == "__main__":
  main()
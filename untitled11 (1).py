# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6m8HK-ne4ZYqSs6FVIi89RBX7bxBMfV
"""

# Commented out IPython magic to ensure Python compatibility.
# hr_policy_chatbot_ultrasafe.py

# %pip install streamlit pypdf sentence-transformers faiss-cpu plotly pandas groq tiktoken
import os
import io
import json
import uuid
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
import time

import streamlit as st
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader

try:
    import faiss  # type: ignore
except Exception:
    faiss = None

try:
    from groq import Groq
except Exception:
    Groq = None

# ------------------------------
# Config
# ------------------------------
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 600
CHUNK_OVERLAP = 150
INDEX_DIR = "./indexes"
DOC_TEXT_DIR = os.path.join(INDEX_DIR, "doc_texts")
DEFAULT_TOP_K = 5

# ------------------------------
# Utilities
# ------------------------------
def ensure_dirs():
    os.makedirs(INDEX_DIR, exist_ok=True)
    os.makedirs(DOC_TEXT_DIR, exist_ok=True)

def hash_bytes(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

def pdf_to_text(pdf_bytes: bytes) -> Tuple[str, List[int]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    texts, page_starts, cursor = [], [], 0
    for page in reader.pages:
        page_starts.append(cursor)
        t = page.extract_text() or ""
        t = "\n".join(line.strip() for line in t.splitlines())
        texts.append(t)
        cursor += len(t) + 1
    return "\n".join(texts), page_starts

def save_doc_text(doc_id: str, text: str):
    path = os.path.join(DOC_TEXT_DIR, f"{doc_id}.txt")
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def load_doc_text(doc_id: str) -> str:
    path = os.path.join(DOC_TEXT_DIR, f"{doc_id}.txt")
    return open(path, "r", encoding="utf-8").read() if os.path.exists(path) else ""

def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    chunks, i, n = [], 0, len(text)
    while i < n:
        j = min(n, i + chunk_size)
        chunks.append((text[i:j], i))
        i = j - overlap if j - overlap > i else j
    return chunks

@dataclass
class ChunkMeta:
    id: str
    doc_id: str
    start: int
    end: int
    page: int
    preview: str

class SimpleVectorIndex:
    def __init__(self, path: str, dim: int):
        self.index_path = path
        self.dim = dim
        self.faiss_index = None
        self.meta, self.id_map = {}, []

    def _create(self):
        self.faiss_index = faiss.IndexFlatIP(self.dim)

    def save(self):
        if self.faiss_index:
            faiss.write_index(self.faiss_index, self.index_path)
        with open(self.index_path + ".meta.json", "w") as f:
            json.dump({k: asdict(v) for k, v in self.meta.items()}, f)
        with open(self.index_path + ".ids", "w") as f:
            json.dump(self.id_map, f)

    def load(self) -> bool:
        if not os.path.exists(self.index_path):
            return False
        self.faiss_index = faiss.read_index(self.index_path)
        with open(self.index_path + ".meta.json") as f:
            self.meta = {k: ChunkMeta(**v) for k, v in json.load(f).items()}
        with open(self.index_path + ".ids") as f:
            self.id_map = json.load(f)
        return True

    def add(self, vectors, metas):
        if not self.faiss_index:
            self._create()
        vectors = vectors.astype("float32")
        vectors /= np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10
        self.faiss_index.add(vectors)
        for m in metas:
            self.id_map.append(m.id)
            self.meta[m.id] = m

    def search(self, query_vec, top_k=DEFAULT_TOP_K):
        q = query_vec.astype("float32")
        q /= np.linalg.norm(q) + 1e-10
        D, I = self.faiss_index.search(q.reshape(1, -1), top_k)
        return [(self.meta[self.id_map[idx]], float(score)) for score, idx in zip(D[0], I[0]) if idx != -1]

@st.cache_resource
def get_embedder():
    return SentenceTransformer(EMBED_MODEL_NAME)

def embed_texts(model, texts, batch_size=8):
    vecs = []
    for i in range(0, len(texts), batch_size):
        batch = [t for t in texts[i:i+batch_size] if t.strip()]
        if batch:
            vecs.append(model.encode(batch, convert_to_numpy=True))
    return np.vstack(vecs) if vecs else np.zeros((0, model.get_sentence_embedding_dimension()))

@st.cache_resource
def get_groq_client():
    api_key = os.getenv("GROQ_API_KEY", "")
    return Groq(api_key=api_key) if api_key and Groq else None

def call_llm(client, sys_prompt, user_prompt, model):
    if not client:
        return "[LLM disabled: no key]"
    res = client.chat.completions.create(
        model=model,
        temperature=0.2,
        max_tokens=600,
        messages=[{"role": "system", "content": sys_prompt},
                  {"role": "user", "content": user_prompt}],
    )
    return res.choices[0].message.content

def append_csv(path: str, row: Dict[str, Any]):
    df = pd.DataFrame([row])
    header = not os.path.exists(path)
    df.to_csv(path, mode="a", header=header, index=False, encoding="utf-8")

SYSTEM_PROMPT = (
    "You are an HR Policy Assistant. Use the CONTEXT to answer clearly and concisely. "
    "If context is incomplete try combining nearby chunks to give a full answer., "
    "but do not just say 'not found'."
    "If the context is irrelevant , respond clearly that you cannot answer because "
    "it is outside HR policy scope. Do not invent information."
)

# ------------------------------
# App
# ------------------------------
def main():
    st.set_page_config(page_title="HR Policy Assistant", page_icon="🧭",layout="wide")
    st.title("🧭 HR Policy Assistant Chatbot")
    st.markdown(
        "🧑‍💼 **Your smart assistant for everything HR — fast, reliable, and always available.**"
    )

    ensure_dirs()

    # 🔒 Define a fixed index name (hidden from user)
    index_name = "hr_index"

    with st.sidebar:
        files = st.file_uploader("Upload HR PDFs", type="pdf", accept_multiple_files=True)
        #index_name = st.text_input("Index name", "hr_index")
        if st.button("📂 Process Document"):
          if not files:
            st.warning("⚠️ Please upload at least one PDF before processing.")
          else :
            embedder = get_embedder()
            index = SimpleVectorIndex(os.path.join(INDEX_DIR, index_name + ".faiss"),
                                      embedder.get_sentence_embedding_dimension())
            index._create()
            for f in files:
                data = f.read()
                doc_id = hash_bytes(data)[:12]
                text, starts = pdf_to_text(data)
                save_doc_text(doc_id, text)
                chunks = chunk_text(text)
                metas, texts = [], []
                for c, s in chunks:
                    cid = str(uuid.uuid4())[:8]
                    metas.append(ChunkMeta(cid, doc_id, s, s+len(c), 0, c[:100]))
                    texts.append(c)
                if texts:
                    vecs = embed_texts(embedder, texts)
                    index.add(vecs, metas)
            index.save()
            st.success("✅ Document processed successfully! You can now ask questions.")

    #q = st.text_input("Ask HR Policy", placeholder="e.g., What are the official working hours?")

    # initialize state if not present
    if "selected_question" not in st.session_state:
      st.session_state.selected_question = ""

    # link the input box with session_state
    q = st.text_input(
    "Ask HR Policy",
    value=st.session_state.selected_question,
    placeholder="e.g., What are the official working hours?",
    key="question_input"
    )

    # --- Sample question chips ---

    st.markdown("Try asking one of these:")
    cols = st.columns(3)
    samples = [
    "What are the official working hours?",
    "How many earned leaves do I get?",
    "What is the notice period for resignation?"
    ]

    # render chips
    for i, sample in enumerate(samples):
      if cols[i % 3].button(sample, key=f"sample_{i}"):
        st.session_state.selected_question = sample

    if st.button("Ask") and q:
        embedder = get_embedder()
        index = SimpleVectorIndex(os.path.join(INDEX_DIR, index_name + ".faiss"),
                                  embedder.get_sentence_embedding_dimension())

        # --- Quick HR relevance check (basic keyword-based) ---
        hr_keywords = ["leave", "holiday", "benefits", "working hours", "policy",
                   "resignation", "transfer", "promotion", "attendance", "overtime"]
        is_hr_related = any(word in q.lower() for word in hr_keywords)

        # ------------------------------
        # Case 1 + 2: No PDF uploaded
        # ------------------------------
        if not index.load():
          if is_hr_related:
            st.warning("📂 No HR policy document has been uploaded yet. Please upload a policy PDF to get accurate answers.")
          else:
            st.info("🤖 I’m an HR Policy Assistant, and I can’t provide an answer for this query since it’s not related to HR policies.")
          return

        # ==============================
        # Relevance Filtering : Case 3+4 : PDF uploaded
        # ==============================

        qvec = embed_texts(embedder, [q])[0]
        hits = index.search(qvec)

        MIN_RELEVANCE = 0.5
        relevant_hits = [(m, s) for m, s in hits if s >= MIN_RELEVANCE]

        if not relevant_hits: # Case 4: PDF uploaded + irrelevant
          st.write("🤖 I’m an HR Policy Assistant, and I can’t provide an answer for this query since it’s not related to HR policies.")
          return

        # Case 3: PDF uploaded + HR-related (with context)
        context_parts = []
        for m, _ in relevant_hits:
            doc_text = load_doc_text(m.doc_id)
            snippet = doc_text[m.start:m.end] if doc_text else m.preview

            # --- NEW: stitch continuation if snippet is cut mid-sentence ---
            if doc_text and not snippet.strip().endswith(('.', '!', '?')):
                extra = doc_text[m.end:m.end+200]  # grab next ~200 chars
                snippet += " " + extra.strip().replace("\n", " ")

            context_parts.append(snippet)

        context = "\n\n".join(context_parts)

        client = get_groq_client()
        ans = call_llm(client, SYSTEM_PROMPT, f"CONTEXT:\n{context}\n\nQ: {q}", "llama-3.1-8b-instant")
        st.write(ans)

        # ==============================
        # Show Sources (clean + interactive)
        # ==============================

        # Only show sources if relevance score > 0.5
        MIN_RELEVANCE = 0.5

        if hits and hits[0][1] > MIN_RELEVANCE:
          best_meta, best_score = hits[0]
          st.markdown("### 📚 Sources")
          st.markdown(f"Showing the **most relevant source** based on your query:")
          with st.expander(f"📌 Page {best_meta.page + 1} (Relevance: {best_score:.2f})"):
            full_text = load_doc_text(best_meta.doc_id)
            snippet = full_text[best_meta.start:best_meta.end] if full_text else best_meta.preview
            st.write(snippet.strip())
        else:
          st.info("ℹ️ No relevant HR policy source found for your question.")

        # ==============================
        # Feedback UI
        # ==============================
        st.markdown("#### Was this answer helpful?")
        col1, col2 = st.columns(2)
        if col1.button("👍 Yes"):
            append_csv("feedback_log.csv", {
                "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
                "question": q,
                "helpful": "Yes",
            })
            st.success("Thanks for your feedback!")
        if col2.button("👎 No"):
            append_csv("feedback_log.csv", {
                "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
                "question": q,
                "helpful": "No",
            })
            st.info("Feedback noted — we'll use it to improve.")

        # ==============================
        # Suggestion UI
        # ==============================
        st.subheader("💡 Policy Suggestions")
        suggestion = st.text_area("Do you have any suggestions or modifications regarding this policy?", "")
        if st.button("Submit Suggestion"):
            if suggestion.strip():
                append_csv("suggestion_log.csv", {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "question": q,
                    "answer": ans,
                    "suggestion": suggestion.strip(),
                })
                st.success("✅ Your suggestion has been recorded and will be reviewed by the HR team.")
            else:
                st.warning("Please enter a suggestion before submitting.")


if __name__ == "__main__":
    main()

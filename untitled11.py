# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6m8HK-ne4ZYqSs6FVIi89RBX7bxBMfV
"""

# hr_policy_chatbot_ultrasafe.py
"""
Ultra-safe HR Policy Assistant Streamlit app.

Changes from previous version:
- Uses a smaller embedding model: "sentence-transformers/paraphrase-MiniLM-L3-v2"
- Embeds in very small batches (batch_size=8) to reduce memory spikes
- Wraps embedding and FAISS operations with try/except to prevent crashes
- Saves doc texts in ./indexes/doc_texts/{doc_id}.txt
"""

import os
import io
import json
import uuid
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple

import streamlit as st
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader

try:
    import faiss  # type: ignore
except Exception:
    faiss = None

try:
    from groq import Groq
except Exception:
    Groq = None

# ------------------------------
# Config
# ------------------------------
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"  
CHUNK_SIZE = 600
CHUNK_OVERLAP = 150
INDEX_DIR = "./indexes"
DOC_TEXT_DIR = os.path.join(INDEX_DIR, "doc_texts")
DEFAULT_TOP_K = 5

# ------------------------------
# Utilities
# ------------------------------
def ensure_dirs():
    os.makedirs(INDEX_DIR, exist_ok=True)
    os.makedirs(DOC_TEXT_DIR, exist_ok=True)

def hash_bytes(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

def pdf_to_text(pdf_bytes: bytes) -> Tuple[str, List[int]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    texts, page_starts, cursor = [], [], 0
    for page in reader.pages:
        page_starts.append(cursor)
        t = page.extract_text() or ""
        t = "\n".join(line.strip() for line in t.splitlines())
        texts.append(t)
        cursor += len(t) + 1
    return "\n".join(texts), page_starts

def save_doc_text(doc_id: str, text: str):
    path = os.path.join(DOC_TEXT_DIR, f"{doc_id}.txt")
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def load_doc_text(doc_id: str) -> str:
    path = os.path.join(DOC_TEXT_DIR, f"{doc_id}.txt")
    return open(path, "r", encoding="utf-8").read() if os.path.exists(path) else ""

def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    chunks, i, n = [], 0, len(text)
    while i < n:
        j = min(n, i + chunk_size)
        chunks.append((text[i:j], i))
        i = j - overlap if j - overlap > i else j
    return chunks

@dataclass
class ChunkMeta:
    id: str
    doc_id: str
    start: int
    end: int
    page: int
    preview: str

class SimpleVectorIndex:
    def __init__(self, path: str, dim: int):
        self.index_path = path
        self.dim = dim
        self.faiss_index = None
        self.meta, self.id_map = {}, []

    def _create(self):
        self.faiss_index = faiss.IndexFlatIP(self.dim)

    def save(self):
        if self.faiss_index:
            faiss.write_index(self.faiss_index, self.index_path)
        with open(self.index_path + ".meta.json", "w") as f:
            json.dump({k: asdict(v) for k, v in self.meta.items()}, f)
        with open(self.index_path + ".ids", "w") as f:
            json.dump(self.id_map, f)

    def load(self) -> bool:
        if not os.path.exists(self.index_path):
            return False
        self.faiss_index = faiss.read_index(self.index_path)
        with open(self.index_path + ".meta.json") as f:
            self.meta = {k: ChunkMeta(**v) for k, v in json.load(f).items()}
        with open(self.index_path + ".ids") as f:
            self.id_map = json.load(f)
        return True

    def add(self, vectors, metas):
        if not self.faiss_index:
            self._create()
        vectors = vectors.astype("float32")
        vectors /= np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10
        self.faiss_index.add(vectors)
        for m in metas:
            self.id_map.append(m.id)
            self.meta[m.id] = m

    def search(self, query_vec, top_k=DEFAULT_TOP_K):
        q = query_vec.astype("float32")
        q /= np.linalg.norm(q) + 1e-10
        D, I = self.faiss_index.search(q.reshape(1, -1), top_k)
        return [(self.meta[self.id_map[idx]], float(score)) for score, idx in zip(D[0], I[0]) if idx != -1]

@st.cache_resource
def get_embedder():
    return SentenceTransformer(EMBED_MODEL_NAME)

def embed_texts(model, texts, batch_size=8):
    vecs = []
    for i in range(0, len(texts), batch_size):
        batch = [t for t in texts[i:i+batch_size] if t.strip()]
        if batch:
            vecs.append(model.encode(batch, convert_to_numpy=True))
    return np.vstack(vecs) if vecs else np.zeros((0, model.get_sentence_embedding_dimension()))

@st.cache_resource
def get_groq_client():
    api_key = os.getenv("GROQ_API_KEY", "")
    return Groq(api_key=api_key) if api_key and Groq else None

def call_llm(client, sys_prompt, user_prompt, model):
    if not client:
        return "[LLM disabled: no key]"
    res = client.chat.completions.create(
        model=model,
        temperature=0.2,
        max_tokens=600,
        messages=[{"role": "system", "content": sys_prompt},
                  {"role": "user", "content": user_prompt}],
    )
    return res.choices[0].message.content

SYSTEM_PROMPT = (
    "You are an HR Policy Assistant. Use the CONTEXT to answer clearly and concisely. "
    "If context is incomplete try combining nearby chunks to give a full answer., "
    "but do not just say 'not found'."
)

# ------------------------------
# App
# ------------------------------
def main():
    st.set_page_config(page_title="HR Policy Assistant", page_icon="ðŸ§­")
    st.title("ðŸ§­ HR Policy Assistant (Ultra-Safe)")
    ensure_dirs()

    with st.sidebar:
        files = st.file_uploader("Upload HR PDFs", type="pdf", accept_multiple_files=True)
        index_name = st.text_input("Index name", "hr_index")
        if st.button("Build Index"):
            embedder = get_embedder()
            index = SimpleVectorIndex(os.path.join(INDEX_DIR, index_name + ".faiss"),
                                      embedder.get_sentence_embedding_dimension())
            index._create()
            for f in files:
                data = f.read()
                doc_id = hash_bytes(data)[:12]
                text, starts = pdf_to_text(data)
                save_doc_text(doc_id, text)
                chunks = chunk_text(text)
                metas, texts = [], []
                for c, s in chunks:
                    cid = str(uuid.uuid4())[:8]
                    metas.append(ChunkMeta(cid, doc_id, s, s+len(c), 0, c[:100]))
                    texts.append(c)
                if texts:
                    vecs = embed_texts(embedder, texts)
                    index.add(vecs, metas)
            index.save()
            st.success("Index built")

    q = st.text_input("Ask HR Policy")
    if st.button("Ask") and q:
        embedder = get_embedder()
        index = SimpleVectorIndex(os.path.join(INDEX_DIR, index_name + ".faiss"),
                                  embedder.get_sentence_embedding_dimension())
        if not index.load():
            st.error("Index not found")
            return
        qvec = embed_texts(embedder, [q])[0]
        hits = index.search(qvec)
        context = "\n\n".join([m.preview for m, _ in hits])
        client = get_groq_client()
        ans = call_llm(client, SYSTEM_PROMPT, f"CONTEXT:\n{context}\n\nQ: {q}", "llama-3.1-8b-instant")
        st.write(ans)

if __name__ == "__main__":
    main()
